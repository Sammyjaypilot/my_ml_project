{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4098e405",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MlflowClient' object has no attribute 'list_registered_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m mlflow.set_tracking_uri(\u001b[33m\"\u001b[39m\u001b[33mhttp://localhost:5000\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m client = MlflowClient()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m model = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist_registered_models\u001b[49m()\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model:\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo registered models found.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'MlflowClient' object has no attribute 'list_registered_models'"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "model = client.list_registered_models()\n",
    "\n",
    "if not model:\n",
    "    print(\"No registered models found.\")\n",
    "else:\n",
    "    for m in model:\n",
    "        print(f\"Model name: {m.name}\")\n",
    "        for version in m.latest_versions:\n",
    "            print(f\" - Version: {version.version}, Stage: {version.current_stage}, Run ID: {version.run_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccaf51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment not found.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = MlflowClient()\n",
    "\n",
    "exp = client.get_experiment_by_name(\"Fever Detection Experiment\")\n",
    "if exp is None:\n",
    "    print(\"Experiment not found.\")\n",
    "else:\n",
    "    exp_id = exp.experiment_id\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[exp_id],\n",
    "        filter_string=\"\",\n",
    "        run_view_type=1,  # active runs; use 0 for all including deleted\n",
    "        max_results=100,\n",
    "        order_by=[\"attributes.start_time DESC\"]\n",
    "    )\n",
    "    for r in runs:\n",
    "        rid = r.info.run_id\n",
    "        name = r.info.run_name\n",
    "        status = r.info.status\n",
    "        mt = r.data.params.get(\"model_types\") or r.data.tags.get(\"model_types\") or \"<no model_types param>\"\n",
    "        print(f\"{rid[:8]}  {name:20}  status={status:10}  model_types={mt}  metrics={r.data.metrics}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c791411",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MlflowClient' object has no attribute 'list_experiments'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m mlflow.set_tracking_uri(\u001b[33m\"\u001b[39m\u001b[33mhttp://localhost:5000\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m client = MlflowClient()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m experiments = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist_experiments\u001b[49m()\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExperiments known to tracking server:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m experiments:\n",
      "\u001b[31mAttributeError\u001b[39m: 'MlflowClient' object has no attribute 'list_experiments'"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "client = MlflowClient()\n",
    "experiments = client.list_experiments()\n",
    "print(\"Experiments known to tracking server:\")\n",
    "for e in experiments:\n",
    "    print(e.experiment_id, e.name, \"lifecycle_stage=\", e.lifecycle_stage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5116f4f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime \n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlflow\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_processor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_data_preprocessor\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtracking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MlflowClient\n",
      "\u001b[31mImportError\u001b[39m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "#Training Pipeline Code\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "from datetime import datetime \n",
    "import mlflow\n",
    "from .data_processor import get_data_preprocessor\n",
    "from .model import Model\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "class TrainingPipeline:\n",
    "    def __init__(self, preprocessor, model_types=None, random_state=None, **model_params):\n",
    "\n",
    "        param_file_path = \"C:/Users/DELL/Desktop/my_ml_project/notebook/params.yaml\"\n",
    "\n",
    "        try:\n",
    "            with open(param_file_path, 'r') as f:\n",
    "                params = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "            self.model_types = model_types or params['model']['model_types']\n",
    "            self.random_state = random_state or params['model'][\"random_forest\", \"logistic_regression\", \"xgboost\", \"svm\", \"knn\", \"decision_tree\"]\n",
    "            self.model_params = model_params or params['model'].get(self.model_types, {})\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not load params.yaml: {e}. Using defaults.\")\n",
    "            self.model_types = model_types or ''\n",
    "            self.random_state = random_state or 42\n",
    "            self.model_params = model_params or {}\n",
    "\n",
    "        #try:\n",
    "        #    #mlflow.sklearn.autolog()\n",
    "        #    self.mlflow_autolog_enabled = True  # Specific to autologging\n",
    "        #    logger.info(\"MLflow autologging enabled successfully\")\n",
    "        #except Exception as e:\n",
    "        #    self.mlflow_autolog_enabled = False\n",
    "        #    logger.warning(f\"MLflow autologging failed: {str(e)}. Manual logging still available.\")\n",
    "        try:\n",
    "            mlflow.sklearn.autolog(disable=True)  # Disable here too\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        self.preprocessor = preprocessor\n",
    "        self.model = Model(\n",
    "            model_types=self.model_types, \n",
    "            random_state=self.random_state, \n",
    "            **self.model_params\n",
    "        )\n",
    "        self.pipeline = None\n",
    "        self.training_history = {}\n",
    "        self.evaluation_results = {}\n",
    "        self._create_pipeline()\n",
    "\n",
    "    def _create_pipeline(self):\n",
    "        \"\"\"\n",
    "        create the pipeline with preprocessor and classifier\n",
    "        \"\"\"\n",
    "        if self.model.classifier is None:\n",
    "            raise ValueError(\"Model classifier is not initialised\")\n",
    "        \n",
    "        self.pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', self.preprocessor),\n",
    "            ('classifier', self.model.classifier)\n",
    "        ])\n",
    "        print(f\"‚úÖ Created pipeline with {self.model_types} classifier\")\n",
    "\n",
    "    def fit(self, X, y, experiment_name= \"Fever Dectection Experiment\", \n",
    "            run_name=None):\n",
    "        \"\"\"\n",
    "        Train the model and track training history\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            self._create_pipeline()\n",
    "\n",
    "        # ensure experiment exists\n",
    "        mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        if run_name is None:\n",
    "            run_name = f\"{self.model_types}_run_{int(datetime.now().strftime('%Y%m%d_%H%M%S'))}\"\n",
    "\n",
    "        #mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "        with mlflow.start_run(run_name=run_name) as run:\n",
    "            \n",
    "            run_id = run.info.run_id\n",
    "            self.training_history[\"mlflow_run_id\"] = run_id\n",
    "\n",
    "            # Log model and training parameters\n",
    "            mlflow.log_param(\"model_type\", self.model_type)\n",
    "            mlflow.log_params(self.model_params)\n",
    "\n",
    "            #start_time = datetime.now()\n",
    "     \n",
    "            self.pipeline.fit(X, y)\n",
    "\n",
    "            training_time = (datetime.now() - start_time).total_seconds()\n",
    "\n",
    "            self.training_history = {\n",
    "                'training_time_seconds': training_time,\n",
    "                'training_samples': len(X),\n",
    "                'features_count': X.shape[1],\n",
    "                'model_types': self.model_types,\n",
    "                'random_state': self.random_state,\n",
    "            'training_timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                mlflow.log_metric(\"training_time\", training_time)\n",
    "                mlflow.log_param(\"feature_count\", X.shape[1])\n",
    "                mlflow.log_param(\"model_types\", self.model_types)\n",
    "                \n",
    "                if hasattr(self, 'pipeline') and self.pipeline is not None:\n",
    "                    mlflow.sklearn.log_model(self.pipeline, \"model\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Manual MLflow logging failed: {str(e)}\")\n",
    "\n",
    "            print(f\"‚úÖ Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if self.pipeline is None:\n",
    "            raise ValueError(\"Model must be trained before making predictions\")\n",
    "        return self.pipeline.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get prediction probabilities\"\"\"\n",
    "        if self.pipeline is None:\n",
    "            raise ValueError(\"Model must be trained before making predictions\")\n",
    "        return self.pipeline.predict_proba(X)\n",
    "\n",
    "    def evaluate(self, X, y, dataset_name='validation'):\n",
    "        \"\"\"\n",
    "        Comprehensive evaluation on a dataset\n",
    "        \n",
    "        Args:\n",
    "            X: Features\n",
    "            y: True labels\n",
    "            dataset_name: Name of the dataset for reporting\n",
    "            \n",
    "        Returns:\n",
    "            dict: Comprehensive evaluation metrics\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            raise ValueError(\"Model must be trained before evaluation\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = self.predict(X)\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "        \n",
    "        # metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y, y_pred),\n",
    "            'precision': precision_score(y, y_pred, average='weighted', zero_division=0),\n",
    "            'recall': recall_score(y, y_pred, average='weighted', zero_division=0),\n",
    "            'f1': f1_score(y, y_pred, average='weighted', zero_division=0),\n",
    "        }\n",
    "        \n",
    "        # ROC-AUC \n",
    "        if len(np.unique(y)) == 2:\n",
    "            metrics['roc_auc'] = roc_auc_score(y, y_pred_proba[:, 1])\n",
    "        else:\n",
    "            metrics['roc_auc'] = roc_auc_score(y, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "        \n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        metrics['confusion_matrix'] = cm.tolist()\n",
    "        \n",
    "        metrics['class_distribution'] = {\n",
    "            'class_0': int((y == 0).sum()),\n",
    "            'class_1': int((y == 1).sum()),\n",
    "            'class_0_percentage': float((y == 0).mean() * 100),\n",
    "            'class_1_percentage': float((y == 1).mean() * 100)\n",
    "        }\n",
    "        \n",
    "        self.evaluation_results[dataset_name] = metrics\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def get_comprehensive_report(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Generate comprehensive training and validation report\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training features\n",
    "            y_train: Training labels\n",
    "            X_val: Validation features\n",
    "            y_val: Validation labels\n",
    "            \n",
    "        Returns:\n",
    "            dict: Comprehensive report with training and validation metrics\n",
    "        \"\"\"\n",
    "        \n",
    "        train_metrics = self.evaluate(X_train, y_train, 'training')\n",
    "        val_metrics = self.evaluate(X_val, y_val, 'validation')\n",
    "        \n",
    "        # Calculate overfitting indicators\n",
    "        accuracy_gap = train_metrics['accuracy'] - val_metrics['accuracy']\n",
    "        f1_gap = train_metrics['f1'] - val_metrics['f1']\n",
    "\n",
    "        # Compute performance gap\n",
    "        perf_gap = {\n",
    "            metric: abs(train_metrics.get(metric, 0) - val_metrics.get(metric, 0))\n",
    "            for metric in [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "        }\n",
    "        \n",
    "        generalization_quality = \"good\"\n",
    "        if perf_gap[\"accuracy\"] > 0.1:\n",
    "            generalization_quality = \"possible_overfit\"\n",
    "\n",
    "        comprehensive_report = {\n",
    "            'training_metadata': self.training_history,\n",
    "            'training_metrics': train_metrics,\n",
    "            'validation_metrics': val_metrics,\n",
    "            'performance_analysis': {\n",
    "                'accuracy_gap': accuracy_gap,\n",
    "                'f1_gap': f1_gap,\n",
    "                'is_overfitting': accuracy_gap > 0.1,  # More than 10% gap\n",
    "                'is_underfitting': train_metrics['accuracy'] < 0.7,  # Poor training performance\n",
    "                'generalization_quality': 'good' if accuracy_gap < 0.05 else 'moderate' if accuracy_gap < 0.1 else 'poor'\n",
    "            },\n",
    "            'model_summary': {\n",
    "                'model_type': self.model_type,\n",
    "                'best_metric': 'accuracy',\n",
    "                'best_score': val_metrics['accuracy'],\n",
    "                'training_status': 'completed'\n",
    "            }\n",
    "        }\n",
    "        # Log model performance characteristics\n",
    "        run_id = self.training_history.get(\"mlflow_run_id\")\n",
    "        if run_id:\n",
    "            with mlflow.start_run(run_id=run_id):\n",
    "                mlflow.set_tag(\"generalization_quality\", comprehensive_report['performance_analysis']['generalization_quality'])\n",
    "                mlflow.set_tag(\"is_overfitting\", str(comprehensive_report['performance_analysis']['is_overfitting']))\n",
    "\n",
    "        return comprehensive_report\n",
    "    \n",
    "    def save_training_report(self, report, filepath=\"reports/training_metrics.json\"):\n",
    "        \"\"\"Save training report to JSON file\"\"\"\n",
    "        import json\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        \n",
    "        # Convert numpy types to Python native types for JSON serialization\n",
    "        def convert_to_serializable(obj):\n",
    "            if isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, dict):\n",
    "                return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "            elif isinstance(obj, (list, tuple)):\n",
    "                return [convert_to_serializable(item) for item in obj]\n",
    "            else:\n",
    "                return obj\n",
    "        \n",
    "        serializable_report = convert_to_serializable(report)\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(serializable_report, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Training report saved to {filepath}\")\n",
    "        run_id = self.training_history.get(\"mlflow_run_id\")\n",
    "        if run_id:\n",
    "            with mlflow.start_run(run_id=run_id):\n",
    "                mlflow.log_artifact(filepath)\n",
    "        return filepath\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the trained pipeline\"\"\"\n",
    "        if self.pipeline is None:\n",
    "            raise ValueError(\"No trained model to save\")\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        \n",
    "        # Save using the model's save method\n",
    "        self.model.pipeline = self.pipeline\n",
    "        self.model.save(filepath)\n",
    "        \n",
    "        print(f\"‚úÖ Model saved to {filepath}\")\n",
    "\n",
    "\n",
    "    # Register the model in MLflow Model Registry\n",
    "    def register_model(self, model_name, transition_to_stage=\"Staging\"):\n",
    "        \"\"\"\n",
    "        Register the model in MLflow Model Registry.\n",
    "        \"\"\"\n",
    "        run_id = self.training_history.get(\"mlflow_run_id\")\n",
    "        if not run_id:\n",
    "            raise ValueError(\"No MLflow run ID found. Train the model first using .fit().\")\n",
    "\n",
    "        client = MlflowClient()\n",
    "        model_uri = f\"runs:/{run_id}/model\"\n",
    "\n",
    "        # Create or update model version\n",
    "        model_version = client.create_model_version(\n",
    "            name=model_name,\n",
    "            source=model_uri,\n",
    "            run_id=run_id\n",
    "        )\n",
    "\n",
    "        print(f\"üì¶ Model version {model_version.version} registered under '{model_name}'\")\n",
    "\n",
    "        if transition_to_stage:\n",
    "            client.transition_model_version_stage(\n",
    "                name=model_name,\n",
    "                version=model_version.version,\n",
    "                stage=transition_to_stage\n",
    "            )\n",
    "            print(f\"üöÄ Transitioned '{model_name}' v{model_version.version} to stage: {transition_to_stage}\")\n",
    "\n",
    "        return model_version\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a trained pipeline\"\"\"\n",
    "        self.model.load(filepath)\n",
    "        self.pipeline = self.model.pipeline\n",
    "        print(f\"‚úÖ Model loaded from {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48191dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model.py\n",
    "# src/ml/model.py\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import yaml\n",
    "\n",
    "XGBOOST_AVAILABLE = True\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, model_types= [\"random_forest\", \"logistic_regression\", \"xgboost\", \"svm\", \"knn\", \"decision_tree\"], random_state=42, **model_params):\n",
    "        \"\"\"\n",
    "        Initialize model with specified type and parameters\n",
    "        \n",
    "        Args:\n",
    "            model_type (str): Type of model to use\n",
    "            random_state (int): Random seed for reproducibility\n",
    "            **model_params: Additional model-specific parameters\n",
    "        \"\"\"\n",
    "        self.pipeline = None\n",
    "        self.model_types = model_types\n",
    "        self.random_state = random_state\n",
    "        self.model_params = model_params\n",
    "        self.classifier = None\n",
    "        self.initialise_model()\n",
    "    \n",
    "    def initialise_model(self):\n",
    "        \"\"\"Initialize the classifier based on model type using parameters from params.yaml\"\"\"\n",
    "        param_path_file = \"C:/Users/DELL/Desktop/my_ml_project/notebook/params.yaml\"\n",
    "\n",
    "        try:\n",
    "            with open(param_path_file, 'r') as f:\n",
    "                params = yaml.safe_load(f)\n",
    "            model_params = params['model']\n",
    "        except:\n",
    "            # Fallback to default parameters if params.yaml not available\n",
    "            model_params = {}\n",
    "        \n",
    "        if self.model_types == 'random_forest':\n",
    "            rf_params = model_params.get('random_forest', {})\n",
    "            self.classifier = RandomForestClassifier(\n",
    "                n_estimators=rf_params.get('n_estimators', self.model_params.get('n_estimators', 100)),\n",
    "                max_depth=rf_params.get('max_depth', self.model_params.get('max_depth', None)),\n",
    "                min_samples_split=rf_params.get('min_samples_split', self.model_params.get('min_samples_split', 2)),\n",
    "                min_samples_leaf=rf_params.get('min_samples_leaf', self.model_params.get('min_samples_leaf', 1)),\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "        elif self.model_types == 'logistic_regression':\n",
    "            lr_params = model_params.get('logistic_regression', {})\n",
    "            self.classifier = LogisticRegression(\n",
    "                C=lr_params.get('C', self.model_params.get('C', 1.0)),\n",
    "                penalty=lr_params.get('penalty', self.model_params.get('penalty', 'l2')),\n",
    "                solver=lr_params.get('solver', self.model_params.get('solver', 'lbfgs')),\n",
    "                max_iter=lr_params.get('max_iter', self.model_params.get('max_iter', 1000)),\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "        elif self.model_types == 'xgboost':\n",
    "            xgb_params = model_params.get('xgboost', {})\n",
    "            self.classifier = XGBClassifier(\n",
    "                n_estimators=xgb_params.get('n_estimators', self.model_params.get('n_estimators', 100)),\n",
    "                max_depth=xgb_params.get('max_depth', self.model_params.get('max_depth', 6)),\n",
    "                learning_rate=xgb_params.get('learning_rate', self.model_params.get('learning_rate', 0.1)),\n",
    "                subsample=xgb_params.get('subsample', self.model_params.get('subsample', 1.0)),\n",
    "                colsample_bytree=xgb_params.get('colsample_bytree', self.model_params.get('colsample_bytree', 1.0)),\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1,\n",
    "                eval_metric='logloss'\n",
    "            )\n",
    "            \n",
    "        elif self.model_types == 'svm':\n",
    "            svm_params = model_params.get('svm', {})\n",
    "            self.classifier = SVC(\n",
    "                C=svm_params.get('C', self.model_params.get('C', 1.0)),\n",
    "                kernel=svm_params.get('kernel', self.model_params.get('kernel', 'rbf')),\n",
    "                probability=True,\n",
    "                random_state=self.random_state\n",
    "            )\n",
    "            \n",
    "        elif self.model_types == 'knn':\n",
    "            knn_params = model_params.get('knn', {})\n",
    "            self.classifier = KNeighborsClassifier(\n",
    "                n_neighbors=knn_params.get('n_neighbors', self.model_params.get('n_neighbors', 5)),\n",
    "                weights=knn_params.get('weights', self.model_params.get('weights', 'uniform')),\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "        elif self.model_types == 'decision_tree':\n",
    "            dt_params = model_params.get('decision_tree', {})\n",
    "            self.classifier = DecisionTreeClassifier(\n",
    "                max_depth=dt_params.get('max_depth', self.model_params.get('max_depth', None)),\n",
    "                min_samples_split=dt_params.get('min_samples_split', self.model_params.get('min_samples_split', 2)),\n",
    "                min_samples_leaf=dt_params.get('min_samples_leaf', self.model_params.get('min_samples_leaf', 1)),\n",
    "                random_state=self.random_state\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            available_models = ['random_forest', 'logistic_regression', 'xgboost', 'svm', 'knn', 'decision_tree']\n",
    "            raise ValueError(f\"Model type '{self.model_types}' not supported. Available models: {available_models}\")\n",
    "        \n",
    "        print(f\"‚úÖ Initialized {self.model_types} classifier with parameters from params.yaml\")\n",
    "        \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get information about the current model\"\"\"\n",
    "        if self.classifier is None:\n",
    "            return {\"status\": \"Model not initialized\"}\n",
    "        \n",
    "        info = {\n",
    "            \"model_types\": self.model_types,\n",
    "            \"classifier\": str(self.classifier.__class__.__name__),\n",
    "            \"parameters\": self.classifier.get_params(),\n",
    "            \"is_fitted\": hasattr(self.classifier, 'classes_')\n",
    "        }\n",
    "        return info\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit the model to training data\"\"\"\n",
    "        if self.pipeline is not None:\n",
    "            self.pipeline.fit(X, y)\n",
    "        elif self.classifier is not None:\n",
    "            self.classifier.fit(X, y)\n",
    "        else:\n",
    "            raise ValueError(\"No model initialized. Call initialise_model() first.\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if self.pipeline is not None:\n",
    "            return self.pipeline.predict(X)\n",
    "        elif self.classifier is not None:\n",
    "            return self.classifier.predict(X)\n",
    "        else:\n",
    "            raise ValueError(\"No model available for prediction\")\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get prediction probabilities\"\"\"\n",
    "        if self.pipeline is not None:\n",
    "            return self.pipeline.predict_proba(X)\n",
    "        elif self.classifier is not None:\n",
    "            # Check if classifier supports predict_proba\n",
    "            if hasattr(self.classifier, 'predict_proba'):\n",
    "                return self.classifier.predict_proba(X)\n",
    "            else:\n",
    "                raise ValueError(f\"{self.model_type} does not support probability predictions\")\n",
    "        else:\n",
    "            raise ValueError(\"No model available for prediction\")\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Return the accuracy score\"\"\"\n",
    "        if self.pipeline is not None:\n",
    "            return self.pipeline.score(X, y)\n",
    "        elif self.classifier is not None:\n",
    "            return self.classifier.score(X, y)\n",
    "        else:\n",
    "            raise ValueError(\"No model available for scoring\")\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save the model/pipeline to file\"\"\"\n",
    "        if self.pipeline is not None:\n",
    "            joblib.dump(self.pipeline, filepath)\n",
    "        elif self.classifier is not None:\n",
    "            joblib.dump(self.classifier, filepath)\n",
    "        else:\n",
    "            raise ValueError(\"No model to save\")\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        \"\"\"Load a model/pipeline from file\"\"\"\n",
    "        loaded_obj = joblib.load(filepath)\n",
    "        \n",
    "        # Determine if it's a pipeline or classifier\n",
    "        if hasattr(loaded_obj, 'steps'):  # It's a pipeline\n",
    "            self.pipeline = loaded_obj\n",
    "            # Extract classifier from pipeline\n",
    "            for name, step in loaded_obj.steps:\n",
    "                if name == 'classifier':\n",
    "                    self.classifier = step\n",
    "                    break\n",
    "        else:  # It's a classifier\n",
    "            self.classifier = loaded_obj\n",
    "        \n",
    "        # Infer model type from the loaded object\n",
    "        self._infer_model_type(loaded_obj)\n",
    "    \n",
    "    def _infer_model_type(self, model_obj):\n",
    "        \"\"\"Infer model type from loaded object\"\"\"\n",
    "        model_class = model_obj.__class__.__name__\n",
    "        \n",
    "        if 'RandomForest' in model_class:\n",
    "            self.model_types = 'random_forest'\n",
    "        elif 'LogisticRegression' in model_class:\n",
    "            self.model_types = 'logistic_regression'\n",
    "        elif 'XGB' in model_class:\n",
    "            self.model_types = 'xgboost'\n",
    "        elif 'SVC' in model_class:\n",
    "            self.model_types = 'svm'\n",
    "        elif 'KNeighbors' in model_class:\n",
    "            self.model_types = 'knn'\n",
    "        elif 'DecisionTree' in model_class:\n",
    "            self.model_types = 'decision_tree'\n",
    "        else:\n",
    "            self.model_types = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ac89490",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlflow\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraining_pipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainingPipeline\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_processor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_data_preprocessor\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_classification\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from src.ml.training_pipeline import TrainingPipeline\n",
    "from src.ml.data_processor import get_data_preprocessor\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Create dataset\n",
    "X, y = make_classification(n_samples=500, n_features=10, n_classes=2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Preprocessor\n",
    "preprocessor = get_data_preprocessor()\n",
    "\n",
    "# 3. Define experiment\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"Fever Detection Experiment\")\n",
    "\n",
    "# 4. Loop over all models\n",
    "model_types = [\"random_forest\", \"logistic_regression\", \"xgboost\", \"svm\", \"knn\", \"decision_tree\"]\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f\"--- Training {model_type} ---\")\n",
    "\n",
    "    # Load model-specific params safely from YAML or defaults\n",
    "    # Example: you could load from your YAML here, but keep only valid keys\n",
    "    # For demo, we use empty dict (defaults)\n",
    "    raw_params = {}  # Replace with YAML params if needed\n",
    "    filtered_params = filter_valid_params(MODEL_CLASS_MAP[model_type], raw_params)\n",
    "\n",
    "    # Initialize pipeline\n",
    "    tp = TrainingPipeline(preprocessor=preprocessor, model_type=model_type, model_params=filtered_params)\n",
    "\n",
    "    # Train & log to MLflow\n",
    "    tp.fit(X_train, y_train, experiment_name=\"Fever Detection Experiment\", run_name=f\"{model_type}_run\")\n",
    "\n",
    "    # Evaluate and save report\n",
    "    report = tp.get_comprehensive_report(X_train, y_train, X_val, y_val)\n",
    "    tp.save_training_report(report, filepath=f\"reports/{model_type}_metrics.json\")\n",
    "\n",
    "    # Register model in MLflow\n",
    "    tp.register_model(model_name=\"FeverSeverityModel\", transition_to_stage=\"Staging\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eda67849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: ['Temperature', 'Fever_Severity', 'Age', 'Gender', 'BMI', 'Headache', 'Body_Ache', 'Fatigue', 'Chronic_Conditions', 'Allergies', 'Smoking_History', 'Alcohol_Consumption', 'Humidity', 'AQI', 'Physical_Activity', 'Diet_Type', 'Heart_Rate', 'Blood_Pressure', 'Previous_Medication', 'Recommended_Medication']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Fever_Severity_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\Desktop\\my_ml_project\\.venv311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Fever_Severity_code'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m df = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33mC:/Users/DELL/Desktop/my_ml_project/notebook/fever.csv\u001b[39m\u001b[33m\"\u001b[39m)   \u001b[38;5;66;03m# or the CSV you use as raw input\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mcolumns:\u001b[39m\u001b[33m\"\u001b[39m, df.columns.tolist())\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtarget dtype:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFever_Severity_code\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.dtype)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33munique values & counts:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, df[\u001b[33m\"\u001b[39m\u001b[33mFever_Severity_code\u001b[39m\u001b[33m\"\u001b[39m].value_counts())\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(df[[\u001b[33m\"\u001b[39m\u001b[33mFever_Severity_code\u001b[39m\u001b[33m\"\u001b[39m]].head())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\Desktop\\my_ml_project\\.venv311\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DELL\\Desktop\\my_ml_project\\.venv311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Fever_Severity_code'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:/Users/DELL/Desktop/my_ml_project/notebook/fever.csv\")   # or the CSV you use as raw input\n",
    "print(\"columns:\", df.columns.tolist())\n",
    "print(\"target dtype:\", df[\"Fever_Severity_code\"].dtype)\n",
    "print(\"unique values & counts:\\n\", df[\"Fever_Severity_code\"].value_counts())\n",
    "print(df[[\"Fever_Severity_code\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db722923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "params_path = Path(\"C:/Users/DELL/Desktop/my_ml_project/notebook/params.yaml\")\n",
    "print(params_path.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2628a48",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# find_experiment.py\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_processor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_params\n\u001b[32m      4\u001b[39m params = load_params()\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîç Checking where experiment_name is in your params.yaml...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "# find_experiment.py\n",
    "from data_processor import load_params\n",
    "\n",
    "params = load_params()\n",
    "print(\"üîç Checking where experiment_name is in your params.yaml...\")\n",
    "\n",
    "# Check all possible locations\n",
    "locations = [\n",
    "    ('training.experiment_name', params.get('training', {}).get('experiment_name')),\n",
    "    ('train.experiment_name', params.get('train', {}).get('experiment_name')), \n",
    "    ('experiment_name (root)', params.get('experiment_name'))\n",
    "]\n",
    "\n",
    "for location, value in locations:\n",
    "    if value:\n",
    "        print(f\"‚úÖ FOUND: {location} = '{value}'\")\n",
    "    else:\n",
    "        print(f\"‚ùå NOT FOUND: {location}\")\n",
    "\n",
    "print(f\"\\nüéØ Your experiment name is: '{params.get('training', {}).get('experiment_name')}'\")\n",
    "print(\"   Look for THIS exact name in MLflow UI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"temperature\": 38.5,\n",
    "                \"age\": 35,\n",
    "                \"bmi\": 24.2,\n",
    "                \"humidity\": 65.0,\n",
    "                \"aqi\": 45.0,\n",
    "                \"heart_rate\": 85.0,\n",
    "                \"gender\": \"Male\",\n",
    "                \"headache\": \"Yes\",\n",
    "                \"body_ache\": \"No\",\n",
    "                \"fatigue\": \"Yes\",\n",
    "                \"chronic_conditions\": \"None\",\n",
    "                \"allergies\": \"No\",\n",
    "                \"smoking_history\": \"Non-smoker\",\n",
    "                \"alcohol_consumption\": \"Occasional\",\n",
    "                \"physical_activity\": \"Moderate\",\n",
    "                \"diet_type\": \"Balanced\",\n",
    "                \"blood_pressure\": \"Normal\",\n",
    "                \"previous_medication\": \"None\",\n",
    "                \"recommended_medication\": \"Paracetamol\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"prediction\": 1,\n",
    "                \"probability\": 0.85,\n",
    "                \"risk_level\": \"High\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"message\": \"FeverSeverity Prediction API is running\",\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"status\": \"healthy\",\n",
    "                \"message\": \"API is running normally\"\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a61c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path to import your ML modules\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n",
    "\n",
    "from src.ml.model_loader import load_production_model\n",
    "from api.models import PatientData, PredictionResponse, HealthResponse, APIResponse\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"FeverSeverity Prediction API\", \n",
    "    version=\"1.00\",\n",
    "    description=\"API for predicting fever severity based on patient data\",\n",
    "    docs_url=\"/docs\",\n",
    "    redoc_url=\"/redoc\"\n",
    ")\n",
    "\n",
    "# Load your production model\n",
    "try:\n",
    "    model = load_production_model()\n",
    "    print(\"‚úÖ Model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    model = None\n",
    "\n",
    "@app.get(\"/\", response_model=APIResponse)\n",
    "async def root():\n",
    "    return APIResponse(message=\"FeverSeverity Prediction API is running\")\n",
    "\n",
    "@app.get(\"/health\", response_model=HealthResponse)\n",
    "async def health_check():\n",
    "    if model is None:\n",
    "        return HealthResponse(status=\"unhealthy\", message=\"Model not loaded\")\n",
    "    return HealthResponse(status=\"healthy\", message=\"API and model are ready\")\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict(patient_data: PatientData):\n",
    "    # Check if model is loaded\n",
    "    if model is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
    "    \n",
    "    try:\n",
    "        # Convert patient data to features array\n",
    "        features = [\n",
    "            patient_data.temperature,\n",
    "            patient_data.age,\n",
    "            patient_data.bmi,\n",
    "            patient_data.humidity,\n",
    "            patient_data.aqi,\n",
    "            patient_data.heart_rate,\n",
    "            patient_data.gender,\n",
    "            patient_data.headache,  \n",
    "            patient_data.body_ache,\n",
    "            patient_data.fatigue,\n",
    "            patient_data.chronic_conditions,\n",
    "            patient_data.allergies,\n",
    "            patient_data.smoking_history,\n",
    "            patient_data.alcohol_consumption,\n",
    "            patient_data.physical_activity,\n",
    "            patient_data.diet_type,\n",
    "            patient_data.blood_pressure,\n",
    "            patient_data.previous_medication,\n",
    "            patient_data.recommended_medication\n",
    "        ]\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict([features])[0]\n",
    "        probability = model.predict_proba([features])[0][1]  # Probability of positive class\n",
    "\n",
    "        # Determine the risk level\n",
    "        risk_level = \"High\" if prediction == 1 else \"Low\"\n",
    "\n",
    "        return PredictionResponse(\n",
    "            prediction=int(prediction),\n",
    "            probability=float(probability),\n",
    "            risk_level=risk_level\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Prediction error: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
